"""
https://contest.yandex.ru/contest/24414/run-report/114206190/

Реализация поисковой системы на базе хэш-таблиц.
В качестве хэш-таблиц используется встроенный в python словарь (dict).

Для поискового индекса используется словарь _word_entries, где ключами являются уникальные слова,
а значениями - словари, содержащие id документов в качестве ключей и количество вхождений слова в документ в качестве значений:

{
  '<уникальное_слово>': {
    <id_документа>: <количество_вхождений>,
    ...
  },
  ..
}

Такая структура данных позволяет за O(1) находить все документы, в которые входит слово.
Сами документы не хранятся, так как в задаче нам нужны только id (номера вхождения).

Внутренняя структура данных {<id_документа>: <количество_вхождений>} также является словарем.
В контексте задачи она могла быть и списком/кортежем, так как нас всегда интересуют все доступные документы.
Однако словарь здесь является более гибким, так как на его основе можно будет выполнять действия над конкретными документами (такие как обновление документа, поиск в конкретном документе и т.д.) быстрее.

Для сортировки документов на выходе используется встроенная сортировка с кастомным ключом, который позволяет отсортировать по убыванию релевантности и возрастанию id (номера вхождения).


Сложность добавления документа по времени: O(s), где s - количество слов в документе, так как требуется посчитать количество вхождений каждого слова и внести каждое уникальное слово в словарь (добавление в словарь O(1));
Сложность поиска по времени: O(k + p*log(p)), где k - количество слов в запросе, p - количество документов с ненулевой релевантностью; складывается из операций получения количества входждений(сложность - O(1)) для каждого уникальные слова в запросе и операции сортировки результатов;

Общая сложность программы по времени: O(n*s + m*(k+p*log(p))), где n - количество вносимых документов, m - количество операций поиска. Складывается из n операций заполнения поискового индекса и m операций поиска, сложности которых указаны выше;
Общая сложность программы по памяти: O(n*r), где r - общее количество уникальных слов во всех документах, так как для каждого уникального слова мы храним количество вхождений в каждый документ;
"""

from collections import Counter
from typing import Union


class SearchEngine:
    def __init__(self):
        self._word_entries: dict[str, dict[int, int]] = {}
        self._last_doc_id: int = 0

    def add_doc(self, doc: str):
        counter = Counter(doc.split())
        self._last_doc_id += 1

        for word, count in counter.items():
            if word not in self._word_entries:
                self._word_entries[word] = {}

            self._word_entries[word][self._last_doc_id] = count

    def search(self, val: str) -> Union[list[int], None]:
        relevance = {}
        for word in set(val.split()):
            if word in self._word_entries:
                for doc_id, count in self._word_entries[word].items():
                    relevance[doc_id] = relevance.get(doc_id, 0) + count

        if not relevance:
            return None

        return [x[0] for x in sorted(relevance.items(), key=lambda x: (x[1], -x[0]), reverse=True)[:5]]


def main():
    n = int(input())
    engine = SearchEngine()

    for _ in range(n):
        doc = input()
        engine.add_doc(doc)

    m = int(input())
    results = []
    for _ in range(m):
        search_val = input()
        result = engine.search(search_val)
        if result:
            results.append(' '.join(map(str, result)))

    print('\n'.join(results))


if __name__ == '__main__':
    main()
